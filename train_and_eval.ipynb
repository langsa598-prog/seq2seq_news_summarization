{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1hrHIwIMdH43s2D26eW3rYUxEcDQmVTf-",
      "authorship_tag": "ABX9TyPVNXFSWnDUGfhhV8gPnvG6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/langsa598-prog/seq2seq_news_summarization/blob/main/train_and_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer ê¸°ë°˜ í•œêµ­ì–´ ë‰´ìŠ¤ ìš”ì•½ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€**\n",
        "\n",
        "## 1. í”„ë¡œì íŠ¸ ê°œìš”\n",
        "\n",
        "ë³¸ í”„ë¡œì íŠ¸ëŠ” Transformer ê¸°ë°˜ Seq2Seq ëª¨ë¸ì„ í™œìš©í•˜ì—¬  \n",
        "í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•œ ìë™ ìš”ì•½ ëª¨ë¸ì„ êµ¬í˜„í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.  \n",
        "\n",
        "Pretrained ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ Fine-tuningì„ ìˆ˜í–‰í•˜ì˜€ìœ¼ë©°,  \n",
        "í•™ìŠµ ì´í›„ Validation ë°ì´í„°ì…‹ì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. í”„ë¡œì íŠ¸ ëª©í‘œ\n",
        "\n",
        "- í•œêµ­ì–´ ë‰´ìŠ¤ ìš”ì•½ì„ ìœ„í•œ Abstractive Summarization ëª¨ë¸ êµ¬í˜„\n",
        "- Pretrained Transformer ëª¨ë¸ Fine-tuning\n",
        "- Validation Set ê¸°ì¤€ ROUGE ì§€í‘œë¥¼ í†µí•œ ì„±ëŠ¥ í‰ê°€\n",
        "- í•™ìŠµ ë° ì¶”ë¡  íŒŒì´í”„ë¼ì¸ êµ¬í˜„\n",
        "\n",
        "---\n",
        "\n",
        "## 3. ë°ì´í„° êµ¬ì„±\n",
        "\n",
        "- ì…ë ¥(Input): ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸\n",
        "- ì¶œë ¥(Target): í•´ë‹¹ ê¸°ì‚¬ì— ëŒ€í•œ ìš”ì•½ ë¬¸ì¥\n",
        "- Train / Validation ë°ì´í„° ë¶„ë¦¬ í›„ í•™ìŠµ ì§„í–‰\n",
        "- ì…ë ¥ ê¸¸ì´ ì œí•œ ë° í† í° truncation ì ìš©\n",
        "\n",
        "---\n",
        "\n",
        "## 4. ëª¨ë¸ í•™ìŠµ\n",
        "\n",
        "- Transformer ê¸°ë°˜ Encoderâ€“Decoder êµ¬ì¡° ì‚¬ìš©\n",
        "- Fine-tuningì„ í†µí•´ ë‰´ìŠ¤ ë„ë©”ì¸ì— ë§ê²Œ ëª¨ë¸ ì ì‘\n",
        "- Beam Search ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„±\n",
        "\n",
        "---\n",
        "\n",
        "## 5. ì„±ëŠ¥ í‰ê°€\n",
        "\n",
        "í•™ìŠµ ì™„ë£Œ í›„ Validation ë°ì´í„°ì— ëŒ€í•´ ìš”ì•½ì„ ìƒì„±í•˜ê³   \n",
        "ROUGE-1, ROUGE-2, ROUGE-L ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ë‹¤.  \n",
        "\n",
        "ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ í•µì‹¬ ë‹¨ì–´ ì¬í˜„ ëŠ¥ë ¥ê³¼ ë¬¸ì¥ ìˆ˜ì¤€ì˜ ìœ ì‚¬ë„ë¥¼ í™•ì¸í•˜ì˜€ë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "â€» ë³¸ íŒŒì¼ì€ ëª¨ë¸ í•™ìŠµ(Training)ê³¼ Validation í‰ê°€(Evaluation)ê¹Œì§€ì˜ ê³¼ì •ì„ í¬í•¨í•œë‹¤."
      ],
      "metadata": {
        "id": "-ATr8CBBklbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pz2qEjDgxzh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/iNES_project\n",
        "!ls"
      ],
      "metadata": {
        "id": "_boq81i-hzUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "VudPyQ65iD9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch tqdm"
      ],
      "metadata": {
        "id": "_DLY08hkiS1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í…ìŠ¤íŠ¸ ìš”ì•½ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (KoBART)**\n",
        "\n",
        "**0. í™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •**\n",
        "* ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ `train.py` íŒŒì´ì¬ íŒŒì¼ë¡œ ì €ì¥\n",
        "* ë°ì´í„° ì²˜ë¦¬ìš© `datasets` ë° ëª¨ë¸ í•™ìŠµìš© `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œ\n",
        "\n",
        "**1. ë°ì´í„° ë¡œë“œ ë° ìƒ˜í”Œë§**\n",
        "* ì „ì²˜ë¦¬ëœ JSON ë°ì´í„°ë¥¼ ë¡œë“œ\n",
        "* ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ í•™ìŠµì„ ìœ„í•´ **ìƒìœ„ 5,000ê°œ ë°ì´í„°ë§Œ ìƒ˜í”Œë§**í•˜ì—¬ `Dataset` ê°ì²´ë¡œ ë³€í™˜\n",
        "\n",
        "**2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ**\n",
        "* í•œêµ­ì–´ ìš”ì•½ì— íŠ¹í™”ëœ **KoBART ëª¨ë¸**`digit82/kobart-summarization`ì„ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì‚¬ìš©\n",
        "* í…ìŠ¤íŠ¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ í† í¬ë‚˜ì´ì €ì™€ Seq2Seq ìƒì„± ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´\n",
        "\n",
        "**3. ë°ì´í„° ì „ì²˜ë¦¬ (í† í¬ë‚˜ì´ì§•)**\n",
        "* **ì›ë¬¸ì€ ìµœëŒ€ 512 í† í°, ìš”ì•½ë¬¸ì€ ìµœëŒ€ 128 í† í°**ìœ¼ë¡œ ê¸¸ì´ë¥¼ ë§ì¶¤ (íŒ¨ë”© ë° ìë¥´ê¸° ì ìš©)\n",
        "* ëª¨ë¸ í•™ìŠµ ì •ë‹µì§€ë¡œ ì“°ê¸° ìœ„í•´ ìš”ì•½ë¬¸ì„ `labels`ë¡œ ì…ë ¥ ë°ì´í„°ì— ì¶”ê°€\n",
        "\n",
        "**4. í•™ìŠµ í™˜ê²½ ì„¤ì • (Training Arguments)**\n",
        "* **ë°°ì¹˜ ì‚¬ì´ì¦ˆ 4, ì—í­(Epoch) 1**ë¡œ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© í•™ìŠµì„ ì„¸íŒ…\n",
        "* **`fp16=True`**ë¥¼ ì ìš©í•´ GPU ë©”ëª¨ë¦¬ë¥¼ ì•„ë¼ê³  í•™ìŠµ ì†ë„ë¥¼ í¬ê²Œ ë†’ì„\n",
        "\n",
        "**5 & 6. í•™ìŠµ ì§„í–‰ ë° ëª¨ë¸ ì €ì¥**\n",
        "* `Trainer`ë¥¼ ì´ìš©í•´ **ëª¨ë¸ í•™ìŠµ(íŒŒì¸íŠœë‹)ì„ ì§„í–‰**\n",
        "* í•™ìŠµì´ ëë‚œ ìµœì¢… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì¶”ë¡ ìš©ìœ¼ë¡œ `./final_model` í´ë”ì— ì €ì¥"
      ],
      "metadata": {
        "id": "yjy8aiOgmXAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "# 1. ë°ì´í„° ë¡œë“œ\n",
        "with open(\"train_processed.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# ë¨¼ì € 5000ê°œë¡œ ìë¥´ê¸°\n",
        "data = data[:5000]\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "# 2. ëª¨ë¸ ì„ íƒ\n",
        "model_name = \"digit82/kobart-summarization\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# 3. í† í¬ë‚˜ì´ì§•\n",
        "def preprocess(example):\n",
        "    # ì›ë¬¸ í† í°í™”\n",
        "    inputs = tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # íƒ€ê²Ÿ ìš”ì•½ í† í°í™”\n",
        "    labels = tokenizer(\n",
        "        example[\"target_text\"],\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
        "\n",
        "# 4. í•™ìŠµ ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "# 5. í•™ìŠµ ì‹œì‘\n",
        "trainer.train()\n",
        "\n",
        "# 6. ëª¨ë¸ ì €ì¥\n",
        "model.save_pretrained(\"./final_model\")\n",
        "tokenizer.save_pretrained(\"./final_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4--YOpfki-pM",
        "outputId": "5a0bf96d-ac8c-4763-b239-0d8bd932fc14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "id": "O1NKYEoyiY32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ëª¨ë¸ í•™ìŠµ(Fine-tuning) ê²°ê³¼\n",
        "\n",
        "**[ì£¼ìš” í•™ìŠµ ì§€í‘œ]**\n",
        "* **ì´ˆê¸° Loss:** 1.843\n",
        "* **ìµœì¢… Train Loss:** 0.5972 (ì„±ê³µì ìœ¼ë¡œ ìˆ˜ë ´)\n",
        "* **Train Runtime:** ì•½ 338ì´ˆ (ì•½ 5ë¶„ 30ì´ˆ ì†Œìš”)\n",
        "* **Total Epochs:** 1\n",
        "* **í•™ìŠµ ì†ë„:** 14.8 samples/sec"
      ],
      "metadata": {
        "id": "TyjLme7Crfw3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **í•™ìŠµ ì™„ë£Œ ëª¨ë¸ ë¡œë“œ ë° ìš”ì•½ í…ŒìŠ¤íŠ¸ (ì¶”ë¡ )**\n",
        "\n",
        "**1. íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°**\n",
        "* í•™ìŠµ í›„ `./final_model` í´ë”ì— ì €ì¥í•´ë‘” **ìµœì¢… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œ**\n",
        "\n",
        "**2. ê²€ì¦ ë°ì´í„°(Validation) ì¤€ë¹„**\n",
        "* í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ `valid_processed.json` íŒŒì¼ì„ ë¡œë“œ\n",
        "* ë°ì´í„° ì¤‘ **ì²« ë²ˆì§¸ ë¬¸ì„œì˜ ì›ë¬¸**(`input_text`)ì„ ê°€ì ¸ì™€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ìœ¼ë¡œ ì‚¬ìš©\n",
        "\n",
        "**3. í…ìŠ¤íŠ¸ ì¶”ë¡  ë° ìš”ì•½ ìƒì„±**\n",
        "* ì›ë¬¸ì„ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í† í¬ë‚˜ì´ì§• (ìµœëŒ€ 512 í† í° ì œí•œ)\n",
        "* **`model.generate()`** í•¨ìˆ˜ë¡œ ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ ìƒì„± (`num_beams=4`ë¥¼ ì ìš©í•´ ë” ìì—°ìŠ¤ëŸ½ê³  í€„ë¦¬í‹° ë†’ì€ ë¬¸ì¥ì„ íƒìƒ‰, ìµœëŒ€ 128 í† í°)\n",
        "* ìƒì„±ëœ ê²°ê³¼ê°’(ID)ì„ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”© (íŠ¹ìˆ˜ í† í° ì œê±°)\n",
        "\n",
        "**4. ê²°ê³¼ í™•ì¸**\n",
        "* ì…ë ¥í•œ ì›ë¬¸ì˜ ì•ë¶€ë¶„ê³¼ ëª¨ë¸ì´ **ì§ì ‘ ìƒì„±í•œ ìš”ì•½ë¬¸ì„ í™”ë©´ì— ì¶œë ¥í•˜ì—¬ ì„±ëŠ¥ì„ í™•ì¸**"
      ],
      "metadata": {
        "id": "1g1Q1Ci-lq2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# í•™ìŠµ ëë‚œ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./final_model\")\n",
        "\n",
        "\n",
        "# ì˜ˆì‹œ: validation ë°ì´í„°ì—ì„œ ì²« ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°\n",
        "import json\n",
        "\n",
        "with open(\"valid_processed.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    valid_data = json.load(f)\n",
        "\n",
        "text = valid_data[0][\"input_text\"]  # ì²« ë²ˆì§¸ ë¬¸ì„œ ì›ë¬¸\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "summary_ids = model.generate(**inputs, max_length=128, num_beams=4)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"ì›ë¬¸:\", text[:200], \"...\")  # ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ\n",
        "print(\"ìƒì„± ìš”ì•½:\", summary)"
      ],
      "metadata": {
        "id": "9a6t6H2Klu4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ```\n",
        "ì›ë¬¸: [ ë°•ì¬ì› ê¸°ì ] 'ëŒ€í•œë¯¼êµ­ 5G í™ë³´ëŒ€ì‚¬'ë¥¼ ìì²˜í•œ ë¬¸ì¬ì¸ ëŒ€í†µë ¹ì€ \"ë„“ê³ , ì²´ì¦ ì—†ëŠ” 'í†µì‹  ê³ ì†ë„ë¡œ'ê°€ 5G\"ë¼ë©° \"ëŒ€í•œë¯¼êµ­ì˜ ëŒ€ì „í™˜ì´ ì´ì œ ë§‰ ì‹œì‘ëë‹¤\"ê³  ê¸°ëŒ€ê°ì„ ë†’ì˜€ë‹¤. ë¬¸ ëŒ€í†µë ¹ì€ 8ì¼ ì„œìš¸ ì˜¬ë¦¼í”½ê³µì›ì—ì„œ ì—´ë¦° 5Gí”ŒëŸ¬ìŠ¤ ì „ëµë°œí‘œì— ì°¸ì„í•´ \"5G ì‹œëŒ€ëŠ” ìš°ë¦¬ê°€ ìƒê°í•˜ê³ , ë§Œë“¤ë©´ ê·¸ê²ƒì´ ì„¸ê³„ í‘œì¤€ì´ ë˜ëŠ” ì‹œëŒ€\"ë¼ë©° \"5GëŠ” ëŒ€í•œë¯¼êµ­ í˜ì‹ ì„±ì¥ì˜  ...\n",
        "ìƒì„± ìš”ì•½: 'ëŒ€í•œë¯¼êµ­ 5G í™ë³´ëŒ€ì‚¬'ë¥¼ ìì²˜í•œ ë¬¸ì¬ì¸ ëŒ€í†µë ¹ì€ 5Gê°€ 4ì°¨ ì‚°ì—…í˜ëª… ì‹œëŒ€ì˜ ê³ ì†ë„ë¡œê°€ ë¼ ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ì—´ì–´ ì¤„ ê²ƒì´ë¼ê³  ê°•ì¡°í–ˆë‹¤.\n",
        "```"
      ],
      "metadata": {
        "id": "vwno_BV4rl-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ê²€ì¦ ë°ì´í„° ì „ì²´ ìš”ì•½ ë° ROUGE í‰ê°€ (CSV ì €ì¥)**\n",
        "\n",
        "**1. ëª¨ë¸ ë¡œë“œ ë° í‰ê°€ í™˜ê²½ ì„¸íŒ…**\n",
        "* í•™ìŠµì´ ëë‚œ ìµœì¢… ëª¨ë¸(`./final_model`)ì„ ë¶ˆëŸ¬ì™€ **GPU(`cuda`)ì— í• ë‹¹**\n",
        "* ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œ(`model.eval()`)ë¡œ ì „í™˜í•˜ê³  ê²€ì¦ìš© ë°ì´í„°(`valid_processed.json`)ë¥¼ ë¡œë“œ\n",
        "\n",
        "**2. ë°°ì¹˜ ë‹¨ìœ„ ìš”ì•½ ìƒì„± (ì†ë„ ìµœì í™”)**\n",
        "* ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ **ë°°ì¹˜ ì‚¬ì´ì¦ˆ 16, ë¹” ì„œì¹˜(`num_beams`) 1, ìµœëŒ€ ì…ë ¥ ê¸¸ì´ 384**ë¡œ ì„¸íŒ…\n",
        "* `tqdm`ìœ¼ë¡œ ì§„í–‰ë¥ ì„ í™•ì¸í•˜ë©°, ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ **`torch.no_grad()`** ìƒíƒœì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ìš”ì•½ë¬¸ì„ ìƒì„±\n",
        "\n",
        "**3. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ (ROUGE Score)**\n",
        "* Hugging Faceì˜ **`evaluate` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•´ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°**\n",
        "* ì‹¤ì œ ì •ë‹µ ìš”ì•½ë¬¸(Reference)ê³¼ ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸(Prediction)ì„ ë¹„êµí•´ ê°ê´€ì ì¸ ìš”ì•½ ì„±ëŠ¥ì„ ìˆ˜ì¹˜í™”\n",
        "\n",
        "**4. í‰ê°€ ê²°ê³¼ CSV íŒŒì¼ ì €ì¥**\n",
        "* ì›ë¬¸, ì •ë‹µ ìš”ì•½ë¬¸, ìƒì„± ìš”ì•½ë¬¸ì„ ë¬¶ì–´ **Pandas DataFrame**ìœ¼ë¡œ ë³€í™˜\n",
        "* ì—‘ì…€ì—ì„œ í•œê¸€ ê¹¨ì§ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ `encoding=\"utf-8-sig\"`ë¥¼ ì ìš©í•˜ì—¬, **`validation_summaries.csv` íŒŒì¼ë¡œ ì €ì¥**"
      ],
      "metadata": {
        "id": "bGWHswwYmiCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²´ validation ìš”ì•½ + ROUGE ê³„ì‚° + CSV ì €ì¥\n",
        "\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "\n",
        "# 1. ëª¨ë¸ ë¡œë“œ\n",
        "model_path = \"./final_model\"  # í•™ìŠµí•œ ëª¨ë¸ ê²½ë¡œ\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2. validation ë°ì´í„° ë¡œë“œ\n",
        "with open(\"valid_processed.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    valid_data = json.load(f)\n",
        "\n",
        "# 3. ë°°ì¹˜ ìš”ì•½ ìƒì„± (ì†ë„ ìµœì í™”)\n",
        "batch_size = 16       # GPU VRAM ì—¬ìœ  ìˆìœ¼ë©´ 32 ê°€ëŠ¥\n",
        "num_beams = 1         # ì†ë„ ìœ„í•´ ë¹”ì„œì¹˜ 1\n",
        "max_input_len = 384   # ì…ë ¥ í† í° ê¸¸ì´ ì¤„ì„\n",
        "max_output_len = 128\n",
        "\n",
        "generated_summaries = []\n",
        "\n",
        "for i in tqdm(range(0, len(valid_data), batch_size), desc=\"Generating summaries\"):\n",
        "    batch = valid_data[i:i+batch_size]\n",
        "    texts = [doc[\"input_text\"] for doc in batch]\n",
        "\n",
        "    # ë°°ì¹˜ í† í¬ë‚˜ì´ì§•\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_input_len,\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_output_len,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    batch_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
        "    generated_summaries.extend(batch_summaries)\n",
        "\n",
        "print(\"ë°°ì¹˜ ìš”ì•½ ì™„ë£Œ, ì´ ìƒì„± ê°œìˆ˜:\", len(generated_summaries))\n",
        "\n",
        "# 4. ROUGE ê³„ì‚°\n",
        "rouge = evaluate.load(\"rouge\")  # â† datasets ëŒ€ì‹  evaluate ì‚¬ìš©\n",
        "\n",
        "references = [doc[\"target_text\"] for doc in valid_data]\n",
        "results = rouge.compute(predictions=generated_summaries, references=references)\n",
        "print(\"ğŸ”¹ ROUGE ê²°ê³¼:\", results)\n",
        "\n",
        "# 5. ê²°ê³¼ CSV ì €ì¥\n",
        "df = pd.DataFrame({\n",
        "    \"input_text\": [doc[\"input_text\"] for doc in valid_data],\n",
        "    \"target_text\": references,\n",
        "    \"generated_summary\": generated_summaries\n",
        "})\n",
        "\n",
        "df.to_csv(\"validation_summaries.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ: validation_summaries.csv\")"
      ],
      "metadata": {
        "id": "H6A5n08XvL5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ê²€ì¦ ë°ì´í„° ì „ì²´ ìš”ì•½ ë° ROUGE í‰ê°€ ê²°ê³¼\n",
        "\n",
        "**[í‰ê°€ ìš”ì•½]**\n",
        "* **ì´ ê²€ì¦ ë°ì´í„° ìˆ˜:** 30,122ê°œ (ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ)\n",
        "* **ì†Œìš” ì‹œê°„:** ì•½ 45ë¶„ 46ì´ˆ\n",
        "\n",
        "**[ROUGE Score ê²°ê³¼]**\n",
        "* **ROUGE-1:** 0.4322 (43.2%)\n",
        "* **ROUGE-2:** 0.1780 (17.8%)\n",
        "* **ROUGE-L:** 0.4132 (41.3%)\n",
        "* **ROUGE-Lsum:** 0.4134 (41.3%)\n",
        "\n",
        "*â€» ìƒì„±ëœ ì „ì²´ ìš”ì•½ ê²°ê³¼ëŠ” `validation_summaries.csv` íŒŒì¼ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.*"
      ],
      "metadata": {
        "id": "VArci2RVsGqK"
      }
    }
  ]
}