{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e2b14151a744f1f87d6500efdb9f8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d161b1e46f74e79a20c7caa18d34130",
              "IPY_MODEL_f8ff2b0192374e309a6e1e98d5324fe8",
              "IPY_MODEL_765fae00238a4530b5efa788caea2913"
            ],
            "layout": "IPY_MODEL_3430f65319184018828bc358dd1082c1"
          }
        },
        "1d161b1e46f74e79a20c7caa18d34130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae2d0b18f37149ab87ba735a14e5a570",
            "placeholder": "​",
            "style": "IPY_MODEL_ee36788fbcb84760bbe5ab00baee6560",
            "value": "Loading weights: 100%"
          }
        },
        "f8ff2b0192374e309a6e1e98d5324fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aef4eb6e347427ba5bd1506d7eb79ab",
            "max": 262,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c0f10555c3f461aa519cc6330171a75",
            "value": 262
          }
        },
        "765fae00238a4530b5efa788caea2913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74ad81dc9ad54d82aa18281dad4fe4b8",
            "placeholder": "​",
            "style": "IPY_MODEL_04db678ec92a41f3a01e188ac11f3610",
            "value": " 262/262 [00:00&lt;00:00, 507.58it/s, Materializing param=model.shared.weight]"
          }
        },
        "3430f65319184018828bc358dd1082c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae2d0b18f37149ab87ba735a14e5a570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee36788fbcb84760bbe5ab00baee6560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7aef4eb6e347427ba5bd1506d7eb79ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c0f10555c3f461aa519cc6330171a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74ad81dc9ad54d82aa18281dad4fe4b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04db678ec92a41f3a01e188ac11f3610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c04ba37ccfca48809be0dd0443c97bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3eea3c4772334db6a05002fc3f96c591",
              "IPY_MODEL_397dfd3773924b38afbb4dfaba9fd96e",
              "IPY_MODEL_4774b650bcdf403e890cc753000ad55b"
            ],
            "layout": "IPY_MODEL_6c947b7fa2ff46a1a43ee706ffa44761"
          }
        },
        "3eea3c4772334db6a05002fc3f96c591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d01386d12a24b19853d483a24ade373",
            "placeholder": "​",
            "style": "IPY_MODEL_a6f5cec0b0414564bd30bd98671c698a",
            "value": "Loading weights: 100%"
          }
        },
        "397dfd3773924b38afbb4dfaba9fd96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1986212963e94b54b7d74683554a5455",
            "max": 262,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8d9756d78ca49e98c43b84dd35764a3",
            "value": 262
          }
        },
        "4774b650bcdf403e890cc753000ad55b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c67f8d94ceca4d35b872527761782c15",
            "placeholder": "​",
            "style": "IPY_MODEL_f6326355d27840378320f5e474055539",
            "value": " 262/262 [00:01&lt;00:00, 374.36it/s, Materializing param=model.shared.weight]"
          }
        },
        "6c947b7fa2ff46a1a43ee706ffa44761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d01386d12a24b19853d483a24ade373": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6f5cec0b0414564bd30bd98671c698a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1986212963e94b54b7d74683554a5455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8d9756d78ca49e98c43b84dd35764a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c67f8d94ceca4d35b872527761782c15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6326355d27840378320f5e474055539": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aad683e0564e41f3a5f6af1dfcc549a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a492fd0b1972498cbb123d839e6b1916",
              "IPY_MODEL_265c3de849244e438f93b09e0d918c25",
              "IPY_MODEL_9f2795732bde46f2b73f10e0c8cb07f7"
            ],
            "layout": "IPY_MODEL_a32084251b5b40e4aba109d00cce21f0"
          }
        },
        "a492fd0b1972498cbb123d839e6b1916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9832af1cf1d7463db4d90c4d3d6781f7",
            "placeholder": "​",
            "style": "IPY_MODEL_3bb5f030b4554520becf7b24974d1479",
            "value": "Downloading builder script: "
          }
        },
        "265c3de849244e438f93b09e0d918c25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c92a1cee4cc84c36bd963be46c77d97c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7a48290d9c549fe84665938493c5959",
            "value": 1
          }
        },
        "9f2795732bde46f2b73f10e0c8cb07f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9169401f166248fb8c9f22eabdbc742e",
            "placeholder": "​",
            "style": "IPY_MODEL_a663ddf5867b44e7b313d2a6a6a90a73",
            "value": " 6.14k/? [00:00&lt;00:00, 437kB/s]"
          }
        },
        "a32084251b5b40e4aba109d00cce21f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9832af1cf1d7463db4d90c4d3d6781f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bb5f030b4554520becf7b24974d1479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c92a1cee4cc84c36bd963be46c77d97c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f7a48290d9c549fe84665938493c5959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9169401f166248fb8c9f22eabdbc742e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a663ddf5867b44e7b313d2a6a6a90a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer 기반 한국어 뉴스 요약 모델 학습 및 평가**\n",
        "\n",
        "## 1. 프로젝트 개요\n",
        "\n",
        "본 프로젝트는 Transformer 기반 Seq2Seq 모델을 활용하여  \n",
        "한국어 뉴스 기사에 대한 자동 요약 모델을 구현하는 것을 목표로 한다.  \n",
        "\n",
        "Pretrained 모델을 기반으로 Fine-tuning을 수행하였으며,  \n",
        "학습 이후 Validation 데이터셋을 통해 모델 성능을 평가하였다.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 프로젝트 목표\n",
        "\n",
        "- 한국어 뉴스 요약을 위한 Abstractive Summarization 모델 구현\n",
        "- Pretrained Transformer 모델 Fine-tuning\n",
        "- Validation Set 기준 ROUGE 지표를 통한 성능 평가\n",
        "- 학습 및 추론 파이프라인 구현\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 데이터 구성\n",
        "\n",
        "- 입력(Input): 뉴스 기사 본문\n",
        "- 출력(Target): 해당 기사에 대한 요약 문장\n",
        "- Train / Validation 데이터 분리 후 학습 진행\n",
        "- 입력 길이 제한 및 토큰 truncation 적용\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 모델 학습\n",
        "\n",
        "- Transformer 기반 Encoder–Decoder 구조 사용\n",
        "- Fine-tuning을 통해 뉴스 도메인에 맞게 모델 적응\n",
        "- Beam Search 기반 텍스트 생성\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 성능 평가\n",
        "\n",
        "학습 완료 후 Validation 데이터에 대해 요약을 생성하고  \n",
        "ROUGE-1, ROUGE-2, ROUGE-L 지표를 통해 모델 성능을 평가하였다.  \n",
        "\n",
        "이를 통해 모델의 핵심 단어 재현 능력과 문장 수준의 유사도를 확인하였다.\n",
        "\n",
        "---\n",
        "\n",
        "※ 본 파일은 모델 학습(Training)과 Validation 평가(Evaluation)까지의 과정을 포함한다."
      ],
      "metadata": {
        "id": "-ATr8CBBklbc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pz2qEjDgxzh",
        "outputId": "78e7f1c7-f98b-4b5d-b104-49d2f5f8c5bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/iNES_project\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_boq81i-hzUX",
        "outputId": "d32268fe-6ce9-4a5d-f318-2da2572a1fe8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/iNES_project\n",
            "final_model    results\t\t     train.py\n",
            "preprocess.py  train_processed.json  valid_processed.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VudPyQ65iD9z",
        "outputId": "535094b6-76de-46a7-bddc-a7eaea3ed7a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DLY08hkiS1J",
        "outputId": "0de0e3ad-0942-4208-fd21-148c55875468"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.10.0+cu128)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.24.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch) (1.3.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **텍스트 요약 모델 학습 스크립트 (KoBART)**\n",
        "\n",
        "**0. 환경 및 라이브러리 설정**\n",
        "* 코드를 실행하면 `train.py` 파이썬 파일로 저장\n",
        "* 데이터 처리용 `datasets` 및 모델 학습용 `transformers` 라이브러리를 로드\n",
        "\n",
        "**1. 데이터 로드 및 샘플링**\n",
        "* 전처리된 JSON 데이터를 로드\n",
        "* 빠른 테스트 학습을 위해 **상위 5,000개 데이터만 샘플링**하여 `Dataset` 객체로 변환\n",
        "\n",
        "**2. 모델 및 토크나이저 로드**\n",
        "* 한국어 요약에 특화된 **KoBART 모델**`digit82/kobart-summarization`을 베이스 모델로 사용\n",
        "* 텍스트 처리를 위한 토크나이저와 Seq2Seq 생성 모델을 불러옴\n",
        "\n",
        "**3. 데이터 전처리 (토크나이징)**\n",
        "* **원문은 최대 512 토큰, 요약문은 최대 128 토큰**으로 길이를 맞춤 (패딩 및 자르기 적용)\n",
        "* 모델 학습 정답지로 쓰기 위해 요약문을 `labels`로 입력 데이터에 추가\n",
        "\n",
        "**4. 학습 환경 설정 (Training Arguments)**\n",
        "* **배치 사이즈 4, 에폭(Epoch) 1**로 빠른 테스트용 학습을 세팅\n",
        "* **`fp16=True`**를 적용해 GPU 메모리를 아끼고 학습 속도를 크게 높임\n",
        "\n",
        "**5 & 6. 학습 진행 및 모델 저장**\n",
        "* `Trainer`를 이용해 **모델 학습(파인튜닝)을 진행**\n",
        "* 학습이 끝난 최종 모델과 토크나이저를 추론용으로 `./final_model` 폴더에 저장"
      ],
      "metadata": {
        "id": "yjy8aiOgmXAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "# 1. 데이터 로드\n",
        "with open(\"train_processed.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# 먼저 5000개로 자르기\n",
        "data = data[:5000]\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "\n",
        "# 2. 모델 선택\n",
        "model_name = \"digit82/kobart-summarization\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# 3. 토크나이징\n",
        "def preprocess(example):\n",
        "    # 원문 토큰화\n",
        "    inputs = tokenizer(\n",
        "        example[\"input_text\"],\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    # 타겟 요약 토큰화\n",
        "    labels = tokenizer(\n",
        "        example[\"target_text\"],\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=False)\n",
        "\n",
        "# 4. 학습 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=100,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset\n",
        ")\n",
        "\n",
        "# 5. 학습 시작\n",
        "trainer.train()\n",
        "\n",
        "# 6. 모델 저장\n",
        "model.save_pretrained(\"./final_model\")\n",
        "tokenizer.save_pretrained(\"./final_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4--YOpfki-pM",
        "outputId": "5a0bf96d-ac8c-4763-b239-0d8bd932fc14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1NKYEoyiY32",
        "outputId": "550e9191-27cd-49cc-fc40-62999388445f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rconfig.json: 0.00B [00:00, ?B/s]\rconfig.json: 1.20kB [00:00, 757kB/s]\n",
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
            "\rtokenizer_config.json:   0% 0.00/295 [00:00<?, ?B/s]\rtokenizer_config.json: 100% 295/295 [00:00<00:00, 1.59MB/s]\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "tokenizer.json: 682kB [00:00, 23.1MB/s]\n",
            "special_tokens_map.json: 100% 109/109 [00:00<00:00, 615kB/s]\n",
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
            "pytorch_model.bin: 100% 496M/496M [00:05<00:00, 96.6MB/s]\n",
            "model.safetensors:   0% 0.00/496M [00:00<?, ?B/s]\n",
            "Loading weights:   0% 0/262 [00:00<?, ?it/s]\u001b[A\n",
            "Loading weights:   0% 1/262 [00:00<00:00, 11244.78it/s, Materializing param=final_logits_bias]\u001b[A\n",
            "Loading weights:   0% 1/262 [00:00<00:00, 5398.07it/s, Materializing param=final_logits_bias] \u001b[A\n",
            "Loading weights:   1% 2/262 [00:00<00:00, 5614.86it/s, Materializing param=model.decoder.embed_positions.weight]\u001b[A\n",
            "Loading weights:   1% 2/262 [00:00<00:00, 4514.86it/s, Materializing param=model.decoder.embed_positions.weight]\u001b[A\n",
            "Loading weights:   1% 3/262 [00:00<00:00, 4753.65it/s, Materializing param=model.decoder.embed_tokens.weight]   \u001b[A\n",
            "Loading weights:   1% 3/262 [00:00<00:00, 4184.54it/s, Materializing param=model.decoder.embed_tokens.weight]\u001b[A\n",
            "Loading weights:   2% 4/262 [00:00<00:00, 4588.95it/s, Materializing param=model.decoder.layernorm_embedding.bias]\u001b[A\n",
            "Loading weights:   2% 4/262 [00:00<00:00, 4197.45it/s, Materializing param=model.decoder.layernorm_embedding.bias]\u001b[A\n",
            "Loading weights:   2% 5/262 [00:00<00:00, 4446.89it/s, Materializing param=model.decoder.layernorm_embedding.weight]\u001b[A\n",
            "Loading weights:   2% 5/262 [00:00<00:00, 4146.21it/s, Materializing param=model.decoder.layernorm_embedding.weight]\u001b[A\n",
            "Loading weights:   2% 6/262 [00:00<00:00, 4438.42it/s, Materializing param=model.decoder.layers.0.encoder_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:   2% 6/262 [00:00<00:00, 4180.37it/s, Materializing param=model.decoder.layers.0.encoder_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:   3% 7/262 [00:00<00:00, 4365.82it/s, Materializing param=model.decoder.layers.0.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:   3% 7/262 [00:00<00:00, 4151.60it/s, Materializing param=model.decoder.layers.0.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:   3% 8/262 [00:00<00:00, 4349.25it/s, Materializing param=model.decoder.layers.0.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:   3% 8/262 [00:00<00:00, 4164.63it/s, Materializing param=model.decoder.layers.0.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:   3% 9/262 [00:00<00:00, 4304.79it/s, Materializing param=model.decoder.layers.0.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:   3% 9/262 [00:00<00:00, 4138.66it/s, Materializing param=model.decoder.layers.0.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:   4% 10/262 [00:00<00:00, 4295.24it/s, Materializing param=model.decoder.layers.0.encoder_attn.q_proj.bias]   \u001b[A\n",
            "Loading weights:   4% 10/262 [00:00<00:00, 4148.67it/s, Materializing param=model.decoder.layers.0.encoder_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:   4% 11/262 [00:00<00:00, 4278.71it/s, Materializing param=model.decoder.layers.0.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:   4% 11/262 [00:00<00:00, 4147.18it/s, Materializing param=model.decoder.layers.0.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:   5% 12/262 [00:00<00:00, 4284.27it/s, Materializing param=model.decoder.layers.0.encoder_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:   5% 12/262 [00:00<00:00, 4164.46it/s, Materializing param=model.decoder.layers.0.encoder_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:   5% 13/262 [00:00<00:00, 4252.20it/s, Materializing param=model.decoder.layers.0.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:   5% 13/262 [00:00<00:00, 4139.22it/s, Materializing param=model.decoder.layers.0.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:   5% 14/262 [00:00<00:00, 4247.09it/s, Materializing param=model.decoder.layers.0.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:   5% 14/262 [00:00<00:00, 4141.35it/s, Materializing param=model.decoder.layers.0.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:   6% 15/262 [00:00<00:00, 4240.10it/s, Materializing param=model.decoder.layers.0.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:   6% 15/262 [00:00<00:00, 4144.02it/s, Materializing param=model.decoder.layers.0.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:   6% 16/262 [00:00<00:00, 4247.13it/s, Materializing param=model.decoder.layers.0.fc1.bias]                      \u001b[A\n",
            "Loading weights:   6% 16/262 [00:00<00:00, 4162.31it/s, Materializing param=model.decoder.layers.0.fc1.bias]\u001b[A\n",
            "Loading weights:   6% 17/262 [00:00<00:00, 4242.22it/s, Materializing param=model.decoder.layers.0.fc1.weight]\u001b[A\n",
            "Loading weights:   6% 17/262 [00:00<00:00, 4157.86it/s, Materializing param=model.decoder.layers.0.fc1.weight]\u001b[A\n",
            "Loading weights:   7% 18/262 [00:00<00:00, 4247.64it/s, Materializing param=model.decoder.layers.0.fc2.bias]  \u001b[A\n",
            "Loading weights:   7% 18/262 [00:00<00:00, 4167.22it/s, Materializing param=model.decoder.layers.0.fc2.bias]\u001b[A\n",
            "Loading weights:   7% 19/262 [00:00<00:00, 4254.54it/s, Materializing param=model.decoder.layers.0.fc2.weight]\u001b[A\n",
            "Loading weights:   7% 19/262 [00:00<00:00, 4175.84it/s, Materializing param=model.decoder.layers.0.fc2.weight]\u001b[A\n",
            "Loading weights:   8% 20/262 [00:00<00:00, 4255.80it/s, Materializing param=model.decoder.layers.0.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:   8% 20/262 [00:00<00:00, 4184.89it/s, Materializing param=model.decoder.layers.0.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:   8% 21/262 [00:00<00:00, 4263.33it/s, Materializing param=model.decoder.layers.0.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:   8% 21/262 [00:00<00:00, 4185.14it/s, Materializing param=model.decoder.layers.0.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:   8% 22/262 [00:00<00:00, 4254.25it/s, Materializing param=model.decoder.layers.0.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:   8% 22/262 [00:00<00:00, 4190.11it/s, Materializing param=model.decoder.layers.0.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:   9% 23/262 [00:00<00:00, 4242.26it/s, Materializing param=model.decoder.layers.0.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:   9% 23/262 [00:00<00:00, 4172.90it/s, Materializing param=model.decoder.layers.0.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:   9% 24/262 [00:00<00:00, 4227.24it/s, Materializing param=model.decoder.layers.0.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:   9% 24/262 [00:00<00:00, 4163.77it/s, Materializing param=model.decoder.layers.0.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  10% 25/262 [00:00<00:00, 4213.52it/s, Materializing param=model.decoder.layers.0.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  10% 25/262 [00:00<00:00, 4147.36it/s, Materializing param=model.decoder.layers.0.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  10% 26/262 [00:00<00:00, 4203.68it/s, Materializing param=model.decoder.layers.0.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  10% 26/262 [00:00<00:00, 4147.72it/s, Materializing param=model.decoder.layers.0.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  10% 27/262 [00:00<00:00, 4205.99it/s, Materializing param=model.decoder.layers.0.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  10% 27/262 [00:00<00:00, 4151.25it/s, Materializing param=model.decoder.layers.0.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  11% 28/262 [00:00<00:00, 4206.62it/s, Materializing param=model.decoder.layers.0.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  11% 28/262 [00:00<00:00, 4156.45it/s, Materializing param=model.decoder.layers.0.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  11% 29/262 [00:00<00:00, 4212.90it/s, Materializing param=model.decoder.layers.0.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  11% 29/262 [00:00<00:00, 4164.86it/s, Materializing param=model.decoder.layers.0.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  11% 30/262 [00:00<00:00, 4206.64it/s, Materializing param=model.decoder.layers.0.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  11% 30/262 [00:00<00:00, 4159.23it/s, Materializing param=model.decoder.layers.0.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  12% 31/262 [00:00<00:00, 4211.28it/s, Materializing param=model.decoder.layers.0.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  12% 31/262 [00:00<00:00, 4166.35it/s, Materializing param=model.decoder.layers.0.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  12% 32/262 [00:00<00:00, 4212.60it/s, Materializing param=model.decoder.layers.1.encoder_attn.k_proj.bias]   \u001b[A\n",
            "Loading weights:  12% 32/262 [00:00<00:00, 4168.90it/s, Materializing param=model.decoder.layers.1.encoder_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  13% 33/262 [00:00<00:00, 4217.82it/s, Materializing param=model.decoder.layers.1.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  13% 33/262 [00:00<00:00, 4175.20it/s, Materializing param=model.decoder.layers.1.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  13% 34/262 [00:00<00:00, 4214.14it/s, Materializing param=model.decoder.layers.1.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  13% 34/262 [00:00<00:00, 4172.58it/s, Materializing param=model.decoder.layers.1.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  13% 35/262 [00:00<00:00, 4218.65it/s, Materializing param=model.decoder.layers.1.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  13% 35/262 [00:00<00:00, 4177.47it/s, Materializing param=model.decoder.layers.1.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  14% 36/262 [00:00<00:00, 4219.03it/s, Materializing param=model.decoder.layers.1.encoder_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  14% 36/262 [00:00<00:00, 4180.49it/s, Materializing param=model.decoder.layers.1.encoder_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  14% 37/262 [00:00<00:00, 4223.99it/s, Materializing param=model.decoder.layers.1.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  14% 37/262 [00:00<00:00, 4186.38it/s, Materializing param=model.decoder.layers.1.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  15% 38/262 [00:00<00:00, 4222.86it/s, Materializing param=model.decoder.layers.1.encoder_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  15% 38/262 [00:00<00:00, 4184.17it/s, Materializing param=model.decoder.layers.1.encoder_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  15% 39/262 [00:00<00:00, 4222.89it/s, Materializing param=model.decoder.layers.1.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  15% 39/262 [00:00<00:00, 4186.47it/s, Materializing param=model.decoder.layers.1.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  15% 40/262 [00:00<00:00, 4226.21it/s, Materializing param=model.decoder.layers.1.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  15% 40/262 [00:00<00:00, 4188.86it/s, Materializing param=model.decoder.layers.1.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  16% 41/262 [00:00<00:00, 4228.65it/s, Materializing param=model.decoder.layers.1.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  16% 41/262 [00:00<00:00, 4194.71it/s, Materializing param=model.decoder.layers.1.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  16% 42/262 [00:00<00:00, 4234.84it/s, Materializing param=model.decoder.layers.1.fc1.bias]                      \u001b[A\n",
            "Loading weights:  16% 42/262 [00:00<00:00, 4197.60it/s, Materializing param=model.decoder.layers.1.fc1.bias]\u001b[A\n",
            "Loading weights:  16% 43/262 [00:00<00:00, 4236.57it/s, Materializing param=model.decoder.layers.1.fc1.weight]\u001b[A\n",
            "Loading weights:  16% 43/262 [00:00<00:00, 4202.71it/s, Materializing param=model.decoder.layers.1.fc1.weight]\u001b[A\n",
            "Loading weights:  17% 44/262 [00:00<00:00, 4241.64it/s, Materializing param=model.decoder.layers.1.fc2.bias]  \u001b[A\n",
            "Loading weights:  17% 44/262 [00:00<00:00, 4208.84it/s, Materializing param=model.decoder.layers.1.fc2.bias]\u001b[A\n",
            "Loading weights:  17% 45/262 [00:00<00:00, 4241.34it/s, Materializing param=model.decoder.layers.1.fc2.weight]\u001b[A\n",
            "Loading weights:  17% 45/262 [00:00<00:00, 4210.96it/s, Materializing param=model.decoder.layers.1.fc2.weight]\u001b[A\n",
            "Loading weights:  18% 46/262 [00:00<00:00, 4248.61it/s, Materializing param=model.decoder.layers.1.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  18% 46/262 [00:00<00:00, 4218.05it/s, Materializing param=model.decoder.layers.1.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  18% 47/262 [00:00<00:00, 4248.27it/s, Materializing param=model.decoder.layers.1.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  18% 47/262 [00:00<00:00, 4217.46it/s, Materializing param=model.decoder.layers.1.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  18% 48/262 [00:00<00:00, 4251.97it/s, Materializing param=model.decoder.layers.1.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  18% 48/262 [00:00<00:00, 4222.54it/s, Materializing param=model.decoder.layers.1.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  19% 49/262 [00:00<00:00, 4253.51it/s, Materializing param=model.decoder.layers.1.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  19% 49/262 [00:00<00:00, 4223.96it/s, Materializing param=model.decoder.layers.1.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  19% 50/262 [00:00<00:00, 4255.50it/s, Materializing param=model.decoder.layers.1.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  19% 50/262 [00:00<00:00, 4227.19it/s, Materializing param=model.decoder.layers.1.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  19% 51/262 [00:00<00:00, 4254.70it/s, Materializing param=model.decoder.layers.1.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  19% 51/262 [00:00<00:00, 4226.29it/s, Materializing param=model.decoder.layers.1.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  20% 52/262 [00:00<00:00, 4254.69it/s, Materializing param=model.decoder.layers.1.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  20% 52/262 [00:00<00:00, 4227.06it/s, Materializing param=model.decoder.layers.1.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  20% 53/262 [00:00<00:00, 4256.38it/s, Materializing param=model.decoder.layers.1.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  20% 53/262 [00:00<00:00, 4227.16it/s, Materializing param=model.decoder.layers.1.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  21% 54/262 [00:00<00:00, 4255.62it/s, Materializing param=model.decoder.layers.1.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  21% 54/262 [00:00<00:00, 4229.16it/s, Materializing param=model.decoder.layers.1.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  21% 55/262 [00:00<00:00, 4254.96it/s, Materializing param=model.decoder.layers.1.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  21% 55/262 [00:00<00:00, 4224.72it/s, Materializing param=model.decoder.layers.1.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  21% 56/262 [00:00<00:00, 4251.63it/s, Materializing param=model.decoder.layers.1.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  21% 56/262 [00:00<00:00, 4226.38it/s, Materializing param=model.decoder.layers.1.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  22% 57/262 [00:00<00:00, 4255.22it/s, Materializing param=model.decoder.layers.1.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  22% 57/262 [00:00<00:00, 4230.00it/s, Materializing param=model.decoder.layers.1.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  22% 58/262 [00:00<00:00, 4254.90it/s, Materializing param=model.decoder.layers.2.encoder_attn.k_proj.bias]   \u001b[A\n",
            "Loading weights:  22% 58/262 [00:00<00:00, 4230.19it/s, Materializing param=model.decoder.layers.2.encoder_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  23% 59/262 [00:00<00:00, 4257.08it/s, Materializing param=model.decoder.layers.2.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  23% 59/262 [00:00<00:00, 4232.90it/s, Materializing param=model.decoder.layers.2.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  23% 60/262 [00:00<00:00, 4254.72it/s, Materializing param=model.decoder.layers.2.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  23% 60/262 [00:00<00:00, 4230.69it/s, Materializing param=model.decoder.layers.2.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  23% 61/262 [00:00<00:00, 4256.83it/s, Materializing param=model.decoder.layers.2.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  23% 61/262 [00:00<00:00, 4233.31it/s, Materializing param=model.decoder.layers.2.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  24% 62/262 [00:00<00:00, 4257.20it/s, Materializing param=model.decoder.layers.2.encoder_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  24% 62/262 [00:00<00:00, 4233.08it/s, Materializing param=model.decoder.layers.2.encoder_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  24% 63/262 [00:00<00:00, 4258.86it/s, Materializing param=model.decoder.layers.2.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  24% 63/262 [00:00<00:00, 4235.92it/s, Materializing param=model.decoder.layers.2.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  24% 64/262 [00:00<00:00, 4257.30it/s, Materializing param=model.decoder.layers.2.encoder_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  24% 64/262 [00:00<00:00, 4235.60it/s, Materializing param=model.decoder.layers.2.encoder_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  25% 65/262 [00:00<00:00, 4258.97it/s, Materializing param=model.decoder.layers.2.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  25% 65/262 [00:00<00:00, 4228.65it/s, Materializing param=model.decoder.layers.2.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  25% 66/262 [00:00<00:00, 4242.38it/s, Materializing param=model.decoder.layers.2.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  25% 66/262 [00:00<00:00, 4216.60it/s, Materializing param=model.decoder.layers.2.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  26% 67/262 [00:00<00:00, 4233.54it/s, Materializing param=model.decoder.layers.2.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  26% 67/262 [00:00<00:00, 4206.04it/s, Materializing param=model.decoder.layers.2.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  26% 68/262 [00:00<00:00, 4220.43it/s, Materializing param=model.decoder.layers.2.fc1.bias]                      \u001b[A\n",
            "Loading weights:  26% 68/262 [00:00<00:00, 4197.21it/s, Materializing param=model.decoder.layers.2.fc1.bias]\u001b[A\n",
            "Loading weights:  26% 69/262 [00:00<00:00, 4212.13it/s, Materializing param=model.decoder.layers.2.fc1.weight]\u001b[A\n",
            "Loading weights:  26% 69/262 [00:00<00:00, 4189.93it/s, Materializing param=model.decoder.layers.2.fc1.weight]\u001b[A\n",
            "Loading weights:  27% 70/262 [00:00<00:00, 4201.81it/s, Materializing param=model.decoder.layers.2.fc2.bias]  \u001b[A\n",
            "Loading weights:  27% 70/262 [00:00<00:00, 2840.57it/s, Materializing param=model.decoder.layers.2.fc2.bias]\u001b[A\n",
            "Loading weights:  27% 71/262 [00:00<00:00, 2855.89it/s, Materializing param=model.decoder.layers.2.fc2.weight]\u001b[A\n",
            "Loading weights:  27% 71/262 [00:00<00:00, 2844.44it/s, Materializing param=model.decoder.layers.2.fc2.weight]\u001b[A\n",
            "Loading weights:  27% 72/262 [00:00<00:00, 2859.89it/s, Materializing param=model.decoder.layers.2.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  27% 72/262 [00:00<00:00, 2849.07it/s, Materializing param=model.decoder.layers.2.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  28% 73/262 [00:00<00:00, 2869.69it/s, Materializing param=model.decoder.layers.2.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  28% 73/262 [00:00<00:00, 2859.98it/s, Materializing param=model.decoder.layers.2.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  28% 74/262 [00:00<00:00, 2882.01it/s, Materializing param=model.decoder.layers.2.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  28% 74/262 [00:00<00:00, 2872.62it/s, Materializing param=model.decoder.layers.2.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  29% 75/262 [00:00<00:00, 2894.67it/s, Materializing param=model.decoder.layers.2.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  29% 75/262 [00:00<00:00, 2884.53it/s, Materializing param=model.decoder.layers.2.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  29% 76/262 [00:00<00:00, 2905.57it/s, Materializing param=model.decoder.layers.2.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  29% 76/262 [00:00<00:00, 2896.07it/s, Materializing param=model.decoder.layers.2.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  29% 77/262 [00:00<00:00, 2917.71it/s, Materializing param=model.decoder.layers.2.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  29% 77/262 [00:00<00:00, 2908.78it/s, Materializing param=model.decoder.layers.2.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  30% 78/262 [00:00<00:00, 2929.98it/s, Materializing param=model.decoder.layers.2.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  30% 78/262 [00:00<00:00, 2921.22it/s, Materializing param=model.decoder.layers.2.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  30% 79/262 [00:00<00:00, 2942.61it/s, Materializing param=model.decoder.layers.2.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  30% 79/262 [00:00<00:00, 2933.96it/s, Materializing param=model.decoder.layers.2.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  31% 80/262 [00:00<00:00, 2952.77it/s, Materializing param=model.decoder.layers.2.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  31% 80/262 [00:00<00:00, 2943.89it/s, Materializing param=model.decoder.layers.2.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  31% 81/262 [00:00<00:00, 2964.20it/s, Materializing param=model.decoder.layers.2.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  31% 81/262 [00:00<00:00, 2955.48it/s, Materializing param=model.decoder.layers.2.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  31% 82/262 [00:00<00:00, 2975.79it/s, Materializing param=model.decoder.layers.2.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  31% 82/262 [00:00<00:00, 2967.14it/s, Materializing param=model.decoder.layers.2.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  32% 83/262 [00:00<00:00, 2987.88it/s, Materializing param=model.decoder.layers.2.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  32% 83/262 [00:00<00:00, 2979.32it/s, Materializing param=model.decoder.layers.2.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  32% 84/262 [00:00<00:00, 2998.38it/s, Materializing param=model.decoder.layers.3.encoder_attn.k_proj.bias]   \u001b[A\n",
            "Loading weights:  32% 84/262 [00:00<00:00, 2990.34it/s, Materializing param=model.decoder.layers.3.encoder_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  32% 85/262 [00:00<00:00, 3009.28it/s, Materializing param=model.decoder.layers.3.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  32% 85/262 [00:00<00:00, 3000.60it/s, Materializing param=model.decoder.layers.3.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  33% 86/262 [00:00<00:00, 3019.83it/s, Materializing param=model.decoder.layers.3.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  33% 86/262 [00:00<00:00, 3011.19it/s, Materializing param=model.decoder.layers.3.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  33% 87/262 [00:00<00:00, 3030.24it/s, Materializing param=model.decoder.layers.3.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  33% 87/262 [00:00<00:00, 3021.71it/s, Materializing param=model.decoder.layers.3.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  34% 88/262 [00:00<00:00, 3040.93it/s, Materializing param=model.decoder.layers.3.encoder_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  34% 88/262 [00:00<00:00, 3031.24it/s, Materializing param=model.decoder.layers.3.encoder_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  34% 89/262 [00:00<00:00, 3049.68it/s, Materializing param=model.decoder.layers.3.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  34% 89/262 [00:00<00:00, 3040.51it/s, Materializing param=model.decoder.layers.3.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  34% 90/262 [00:00<00:00, 3059.03it/s, Materializing param=model.decoder.layers.3.encoder_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  34% 90/262 [00:00<00:00, 3050.67it/s, Materializing param=model.decoder.layers.3.encoder_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  35% 91/262 [00:00<00:00, 3068.65it/s, Materializing param=model.decoder.layers.3.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  35% 91/262 [00:00<00:00, 3060.46it/s, Materializing param=model.decoder.layers.3.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  35% 92/262 [00:00<00:00, 3078.81it/s, Materializing param=model.decoder.layers.3.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  35% 92/262 [00:00<00:00, 3070.53it/s, Materializing param=model.decoder.layers.3.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  35% 93/262 [00:00<00:00, 3086.98it/s, Materializing param=model.decoder.layers.3.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  35% 93/262 [00:00<00:00, 3078.47it/s, Materializing param=model.decoder.layers.3.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  36% 94/262 [00:00<00:00, 3095.23it/s, Materializing param=model.decoder.layers.3.fc1.bias]                      \u001b[A\n",
            "Loading weights:  36% 94/262 [00:00<00:00, 3087.33it/s, Materializing param=model.decoder.layers.3.fc1.bias]\u001b[A\n",
            "Loading weights:  36% 95/262 [00:00<00:00, 3105.61it/s, Materializing param=model.decoder.layers.3.fc1.weight]\u001b[A\n",
            "Loading weights:  36% 95/262 [00:00<00:00, 3097.79it/s, Materializing param=model.decoder.layers.3.fc1.weight]\u001b[A\n",
            "Loading weights:  37% 96/262 [00:00<00:00, 3115.98it/s, Materializing param=model.decoder.layers.3.fc2.bias]  \u001b[A\n",
            "Loading weights:  37% 96/262 [00:00<00:00, 3108.19it/s, Materializing param=model.decoder.layers.3.fc2.bias]\u001b[A\n",
            "Loading weights:  37% 97/262 [00:00<00:00, 3124.62it/s, Materializing param=model.decoder.layers.3.fc2.weight]\u001b[A\n",
            "Loading weights:  37% 97/262 [00:00<00:00, 3117.01it/s, Materializing param=model.decoder.layers.3.fc2.weight]\u001b[A\n",
            "Loading weights:  37% 98/262 [00:00<00:00, 3134.61it/s, Materializing param=model.decoder.layers.3.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  37% 98/262 [00:00<00:00, 3126.24it/s, Materializing param=model.decoder.layers.3.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  38% 99/262 [00:00<00:00, 3143.68it/s, Materializing param=model.decoder.layers.3.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  38% 99/262 [00:00<00:00, 3135.63it/s, Materializing param=model.decoder.layers.3.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  38% 100/262 [00:00<00:00, 3152.36it/s, Materializing param=model.decoder.layers.3.self_attn.k_proj.bias] \u001b[A\n",
            "Loading weights:  38% 100/262 [00:00<00:00, 3144.39it/s, Materializing param=model.decoder.layers.3.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  39% 101/262 [00:00<00:00, 3161.14it/s, Materializing param=model.decoder.layers.3.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  39% 101/262 [00:00<00:00, 3152.16it/s, Materializing param=model.decoder.layers.3.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  39% 102/262 [00:00<00:00, 3168.37it/s, Materializing param=model.decoder.layers.3.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  39% 102/262 [00:00<00:00, 3159.90it/s, Materializing param=model.decoder.layers.3.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  39% 103/262 [00:00<00:00, 3176.08it/s, Materializing param=model.decoder.layers.3.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  39% 103/262 [00:00<00:00, 3168.18it/s, Materializing param=model.decoder.layers.3.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  40% 104/262 [00:00<00:00, 3184.39it/s, Materializing param=model.decoder.layers.3.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  40% 104/262 [00:00<00:00, 3175.91it/s, Materializing param=model.decoder.layers.3.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  40% 105/262 [00:00<00:00, 3191.76it/s, Materializing param=model.decoder.layers.3.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  40% 105/262 [00:00<00:00, 3182.74it/s, Materializing param=model.decoder.layers.3.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  40% 106/262 [00:00<00:00, 3198.17it/s, Materializing param=model.decoder.layers.3.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  40% 106/262 [00:00<00:00, 3190.36it/s, Materializing param=model.decoder.layers.3.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  41% 107/262 [00:00<00:00, 3204.36it/s, Materializing param=model.decoder.layers.3.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  41% 107/262 [00:00<00:00, 3196.72it/s, Materializing param=model.decoder.layers.3.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  41% 108/262 [00:00<00:00, 3212.29it/s, Materializing param=model.decoder.layers.3.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  41% 108/262 [00:00<00:00, 3204.66it/s, Materializing param=model.decoder.layers.3.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  42% 109/262 [00:00<00:00, 3220.57it/s, Materializing param=model.decoder.layers.3.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  42% 109/262 [00:00<00:00, 3212.94it/s, Materializing param=model.decoder.layers.3.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  42% 110/262 [00:00<00:00, 3227.49it/s, Materializing param=model.decoder.layers.4.encoder_attn.k_proj.bias]   \u001b[A\n",
            "Loading weights:  42% 110/262 [00:00<00:00, 3219.88it/s, Materializing param=model.decoder.layers.4.encoder_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  42% 111/262 [00:00<00:00, 3234.43it/s, Materializing param=model.decoder.layers.4.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  42% 111/262 [00:00<00:00, 3226.75it/s, Materializing param=model.decoder.layers.4.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  43% 112/262 [00:00<00:00, 3241.62it/s, Materializing param=model.decoder.layers.4.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  43% 112/262 [00:00<00:00, 3234.23it/s, Materializing param=model.decoder.layers.4.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  43% 113/262 [00:00<00:00, 3249.24it/s, Materializing param=model.decoder.layers.4.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  43% 113/262 [00:00<00:00, 3241.66it/s, Materializing param=model.decoder.layers.4.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  44% 114/262 [00:00<00:00, 3255.23it/s, Materializing param=model.decoder.layers.4.encoder_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  44% 114/262 [00:00<00:00, 3247.93it/s, Materializing param=model.decoder.layers.4.encoder_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  44% 115/262 [00:00<00:00, 3261.53it/s, Materializing param=model.decoder.layers.4.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  44% 115/262 [00:00<00:00, 3253.92it/s, Materializing param=model.decoder.layers.4.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  44% 116/262 [00:00<00:00, 3268.61it/s, Materializing param=model.decoder.layers.4.encoder_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  44% 116/262 [00:00<00:00, 3261.18it/s, Materializing param=model.decoder.layers.4.encoder_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  45% 117/262 [00:00<00:00, 3275.77it/s, Materializing param=model.decoder.layers.4.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  45% 117/262 [00:00<00:00, 3268.46it/s, Materializing param=model.decoder.layers.4.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  45% 118/262 [00:00<00:00, 3283.08it/s, Materializing param=model.decoder.layers.4.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  45% 118/262 [00:00<00:00, 3274.68it/s, Materializing param=model.decoder.layers.4.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  45% 119/262 [00:00<00:00, 3288.67it/s, Materializing param=model.decoder.layers.4.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  45% 119/262 [00:00<00:00, 3279.97it/s, Materializing param=model.decoder.layers.4.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  46% 120/262 [00:00<00:00, 3292.86it/s, Materializing param=model.decoder.layers.4.fc1.bias]                      \u001b[A\n",
            "Loading weights:  46% 120/262 [00:00<00:00, 3285.70it/s, Materializing param=model.decoder.layers.4.fc1.bias]\u001b[A\n",
            "Loading weights:  46% 121/262 [00:00<00:00, 3300.09it/s, Materializing param=model.decoder.layers.4.fc1.weight]\u001b[A\n",
            "Loading weights:  46% 121/262 [00:00<00:00, 3293.04it/s, Materializing param=model.decoder.layers.4.fc1.weight]\u001b[A\n",
            "Loading weights:  47% 122/262 [00:00<00:00, 3307.30it/s, Materializing param=model.decoder.layers.4.fc2.bias]  \u001b[A\n",
            "Loading weights:  47% 122/262 [00:00<00:00, 3300.49it/s, Materializing param=model.decoder.layers.4.fc2.bias]\u001b[A\n",
            "Loading weights:  47% 123/262 [00:00<00:00, 3313.29it/s, Materializing param=model.decoder.layers.4.fc2.weight]\u001b[A\n",
            "Loading weights:  47% 123/262 [00:00<00:00, 3306.31it/s, Materializing param=model.decoder.layers.4.fc2.weight]\u001b[A\n",
            "Loading weights:  47% 124/262 [00:00<00:00, 3319.78it/s, Materializing param=model.decoder.layers.4.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  47% 124/262 [00:00<00:00, 3312.87it/s, Materializing param=model.decoder.layers.4.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  48% 125/262 [00:00<00:00, 3326.74it/s, Materializing param=model.decoder.layers.4.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  48% 125/262 [00:00<00:00, 3319.98it/s, Materializing param=model.decoder.layers.4.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  48% 126/262 [00:00<00:00, 3333.87it/s, Materializing param=model.decoder.layers.4.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  48% 126/262 [00:00<00:00, 3327.03it/s, Materializing param=model.decoder.layers.4.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  48% 127/262 [00:00<00:00, 3338.95it/s, Materializing param=model.decoder.layers.4.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  48% 127/262 [00:00<00:00, 3332.33it/s, Materializing param=model.decoder.layers.4.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  49% 128/262 [00:00<00:00, 3345.09it/s, Materializing param=model.decoder.layers.4.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  49% 128/262 [00:00<00:00, 3338.15it/s, Materializing param=model.decoder.layers.4.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  49% 129/262 [00:00<00:00, 3351.10it/s, Materializing param=model.decoder.layers.4.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  49% 129/262 [00:00<00:00, 3344.16it/s, Materializing param=model.decoder.layers.4.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  50% 130/262 [00:00<00:00, 3357.08it/s, Materializing param=model.decoder.layers.4.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  50% 130/262 [00:00<00:00, 3350.27it/s, Materializing param=model.decoder.layers.4.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  50% 131/262 [00:00<00:00, 3363.33it/s, Materializing param=model.decoder.layers.4.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  50% 131/262 [00:00<00:00, 3355.34it/s, Materializing param=model.decoder.layers.4.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  50% 132/262 [00:00<00:00, 3367.85it/s, Materializing param=model.decoder.layers.4.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  50% 132/262 [00:00<00:00, 3360.53it/s, Materializing param=model.decoder.layers.4.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  51% 133/262 [00:00<00:00, 3373.18it/s, Materializing param=model.decoder.layers.4.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  51% 133/262 [00:00<00:00, 3366.34it/s, Materializing param=model.decoder.layers.4.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  51% 134/262 [00:00<00:00, 3379.05it/s, Materializing param=model.decoder.layers.4.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  51% 134/262 [00:00<00:00, 3372.13it/s, Materializing param=model.decoder.layers.4.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  52% 135/262 [00:00<00:00, 3384.95it/s, Materializing param=model.decoder.layers.4.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  52% 135/262 [00:00<00:00, 3378.12it/s, Materializing param=model.decoder.layers.4.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  52% 136/262 [00:00<00:00, 3389.50it/s, Materializing param=model.decoder.layers.5.encoder_attn.k_proj.bias]   \u001b[A\n",
            "Loading weights:  52% 136/262 [00:00<00:00, 3382.68it/s, Materializing param=model.decoder.layers.5.encoder_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  52% 137/262 [00:00<00:00, 3394.01it/s, Materializing param=model.decoder.layers.5.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  52% 137/262 [00:00<00:00, 3387.21it/s, Materializing param=model.decoder.layers.5.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  53% 138/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  53% 138/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  53% 138/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  53% 139/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  53% 139/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  53% 140/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  53% 140/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  54% 141/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  54% 141/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  54% 142/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  54% 142/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  55% 143/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  55% 143/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  55% 144/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  55% 144/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  55% 145/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  55% 145/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.encoder_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  56% 146/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.fc1.bias]                      \u001b[A\n",
            "Loading weights:  56% 146/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.fc1.bias]\u001b[A\n",
            "Loading weights:  56% 147/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.fc1.weight]\u001b[A\n",
            "Loading weights:  56% 147/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.fc1.weight]\u001b[A\n",
            "Loading weights:  56% 148/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.fc2.bias]  \u001b[A\n",
            "Loading weights:  56% 148/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.fc2.bias]\u001b[A\n",
            "Loading weights:  57% 149/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.fc2.weight]\u001b[A\n",
            "Loading weights:  57% 149/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.fc2.weight]\u001b[A\n",
            "Loading weights:  57% 150/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  57% 150/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  58% 151/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  58% 151/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  58% 152/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  58% 152/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  58% 153/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  58% 153/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  59% 154/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  59% 154/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  59% 155/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  59% 155/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  60% 156/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  60% 156/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  60% 157/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  60% 157/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  60% 158/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  60% 158/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  61% 159/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  61% 159/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  61% 160/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  61% 160/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  61% 161/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  61% 161/262 [00:00<00:00, 1095.51it/s, Materializing param=model.decoder.layers.5.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  62% 162/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.embed_positions.weight]              \u001b[A\n",
            "Loading weights:  62% 162/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.embed_positions.weight]\u001b[A\n",
            "Loading weights:  62% 163/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.embed_tokens.weight]   \u001b[A\n",
            "Loading weights:  62% 163/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.embed_tokens.weight]\u001b[A\n",
            "Loading weights:  63% 164/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layernorm_embedding.bias]\u001b[A\n",
            "Loading weights:  63% 164/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layernorm_embedding.bias]\u001b[A\n",
            "Loading weights:  63% 165/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layernorm_embedding.weight]\u001b[A\n",
            "Loading weights:  63% 165/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layernorm_embedding.weight]\u001b[A\n",
            "Loading weights:  63% 166/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.fc1.bias]         \u001b[A\n",
            "Loading weights:  63% 166/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.fc1.bias]\u001b[A\n",
            "Loading weights:  64% 167/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.fc1.weight]\u001b[A\n",
            "Loading weights:  64% 167/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.fc1.weight]\u001b[A\n",
            "Loading weights:  64% 168/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.fc2.bias]  \u001b[A\n",
            "Loading weights:  64% 168/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.fc2.bias]\u001b[A\n",
            "Loading weights:  65% 169/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.fc2.weight]\u001b[A\n",
            "Loading weights:  65% 169/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.fc2.weight]\u001b[A\n",
            "Loading weights:  65% 170/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  65% 170/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  65% 171/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  65% 171/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  66% 172/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  66% 172/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  66% 173/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  66% 173/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  66% 174/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  66% 174/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  67% 175/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  67% 175/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  67% 176/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  67% 176/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  68% 177/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  68% 177/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  68% 178/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  68% 178/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  68% 179/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  68% 179/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  69% 180/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  69% 180/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  69% 181/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  69% 181/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.0.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  69% 182/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.fc1.bias]                   \u001b[A\n",
            "Loading weights:  69% 182/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.fc1.bias]\u001b[A\n",
            "Loading weights:  70% 183/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.fc1.weight]\u001b[A\n",
            "Loading weights:  70% 183/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.fc1.weight]\u001b[A\n",
            "Loading weights:  70% 184/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.fc2.bias]  \u001b[A\n",
            "Loading weights:  70% 184/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.fc2.bias]\u001b[A\n",
            "Loading weights:  71% 185/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.fc2.weight]\u001b[A\n",
            "Loading weights:  71% 185/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.fc2.weight]\u001b[A\n",
            "Loading weights:  71% 186/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  71% 186/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  71% 187/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  71% 187/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  72% 188/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  72% 188/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  72% 189/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  72% 189/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  73% 190/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  73% 190/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  73% 191/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  73% 191/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  73% 192/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  73% 192/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  74% 193/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  74% 193/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  74% 194/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  74% 194/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  74% 195/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  74% 195/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  75% 196/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  75% 196/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  75% 197/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  75% 197/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.1.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  76% 198/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.fc1.bias]                   \u001b[A\n",
            "Loading weights:  76% 198/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.fc1.bias]\u001b[A\n",
            "Loading weights:  76% 199/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.fc1.weight]\u001b[A\n",
            "Loading weights:  76% 199/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.fc1.weight]\u001b[A\n",
            "Loading weights:  76% 200/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.fc2.bias]  \u001b[A\n",
            "Loading weights:  76% 200/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.fc2.bias]\u001b[A\n",
            "Loading weights:  77% 201/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.fc2.weight]\u001b[A\n",
            "Loading weights:  77% 201/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.fc2.weight]\u001b[A\n",
            "Loading weights:  77% 202/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  77% 202/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  77% 203/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  77% 203/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  78% 204/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  78% 204/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  78% 205/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  78% 205/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  79% 206/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  79% 206/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  79% 207/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  79% 207/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  79% 208/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  79% 208/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  80% 209/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  80% 209/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  80% 210/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  80% 210/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  81% 211/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  81% 211/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  81% 212/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  81% 212/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  81% 213/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  81% 213/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.2.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  82% 214/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.fc1.bias]                   \u001b[A\n",
            "Loading weights:  82% 214/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.fc1.bias]\u001b[A\n",
            "Loading weights:  82% 215/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.fc1.weight]\u001b[A\n",
            "Loading weights:  82% 215/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.fc1.weight]\u001b[A\n",
            "Loading weights:  82% 216/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.fc2.bias]  \u001b[A\n",
            "Loading weights:  82% 216/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.fc2.bias]\u001b[A\n",
            "Loading weights:  83% 217/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.fc2.weight]\u001b[A\n",
            "Loading weights:  83% 217/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.fc2.weight]\u001b[A\n",
            "Loading weights:  83% 218/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  83% 218/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  84% 219/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  84% 219/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  84% 220/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  84% 220/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  84% 221/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  84% 221/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  85% 222/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  85% 222/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  85% 223/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  85% 223/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  85% 224/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  85% 224/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  86% 225/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  86% 225/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  86% 226/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  86% 226/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  87% 227/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  87% 227/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  87% 228/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  87% 228/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  87% 229/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  87% 229/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.3.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  88% 230/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.fc1.bias]                   \u001b[A\n",
            "Loading weights:  88% 230/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.fc1.bias]\u001b[A\n",
            "Loading weights:  88% 231/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.fc1.weight]\u001b[A\n",
            "Loading weights:  88% 231/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.fc1.weight]\u001b[A\n",
            "Loading weights:  89% 232/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.fc2.bias]  \u001b[A\n",
            "Loading weights:  89% 232/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.fc2.bias]\u001b[A\n",
            "Loading weights:  89% 233/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.fc2.weight]\u001b[A\n",
            "Loading weights:  89% 233/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.fc2.weight]\u001b[A\n",
            "Loading weights:  89% 234/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  89% 234/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  90% 235/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  90% 235/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  90% 236/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  90% 236/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  90% 237/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  90% 237/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  91% 238/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  91% 238/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  91% 239/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  91% 239/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  92% 240/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  92% 240/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  92% 241/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  92% 241/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  92% 242/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  92% 242/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  93% 243/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  93% 243/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  93% 244/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  93% 244/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  94% 245/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  94% 245/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.4.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights:  94% 246/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.5.fc1.bias]                   \u001b[A\n",
            "Loading weights:  94% 246/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.5.fc1.bias]\u001b[A\n",
            "Loading weights:  94% 247/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.5.fc1.weight]\u001b[A\n",
            "Loading weights:  94% 247/262 [00:00<00:00, 1095.51it/s, Materializing param=model.encoder.layers.5.fc1.weight]\u001b[A\n",
            "Loading weights:  95% 248/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.fc1.weight]\u001b[A\n",
            "Loading weights:  95% 248/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.fc2.bias]  \u001b[A\n",
            "Loading weights:  95% 248/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.fc2.bias]\u001b[A\n",
            "Loading weights:  95% 249/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.fc2.weight]\u001b[A\n",
            "Loading weights:  95% 249/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.fc2.weight]\u001b[A\n",
            "Loading weights:  95% 250/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  95% 250/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.final_layer_norm.bias]\u001b[A\n",
            "Loading weights:  96% 251/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  96% 251/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.final_layer_norm.weight]\u001b[A\n",
            "Loading weights:  96% 252/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.k_proj.bias]  \u001b[A\n",
            "Loading weights:  96% 252/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  97% 253/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  97% 253/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  97% 254/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  97% 254/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.out_proj.bias]\u001b[A\n",
            "Loading weights:  97% 255/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  97% 255/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.out_proj.weight]\u001b[A\n",
            "Loading weights:  98% 256/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.q_proj.bias]    \u001b[A\n",
            "Loading weights:  98% 256/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  98% 257/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  98% 257/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  98% 258/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  98% 258/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  99% 259/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  99% 259/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  99% 260/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights:  99% 260/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn_layer_norm.bias]\u001b[A\n",
            "Loading weights: 100% 261/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights: 100% 261/262 [00:00<00:00, 1018.57it/s, Materializing param=model.encoder.layers.5.self_attn_layer_norm.weight]\u001b[A\n",
            "Loading weights: 100% 262/262 [00:00<00:00, 1018.57it/s, Materializing param=model.shared.weight]                               \u001b[A\n",
            "Loading weights: 100% 262/262 [00:00<00:00, 1068.42it/s, Materializing param=model.shared.weight]\n",
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "\n",
            "Map:   0% 0/5000 [00:00<?, ? examples/s]\u001b[A\n",
            "Map:   2% 81/5000 [00:00<00:06, 795.59 examples/s]\u001b[A\n",
            "Map:   4% 201/5000 [00:00<00:06, 794.84 examples/s]\u001b[A\n",
            "Map:   6% 311/5000 [00:00<00:06, 762.51 examples/s]\u001b[A\n",
            "Map:   8% 424/5000 [00:00<00:06, 755.17 examples/s]\u001b[A\n",
            "Map:  10% 513/5000 [00:00<00:06, 693.45 examples/s]\u001b[A\n",
            "Map:  12% 585/5000 [00:00<00:08, 547.72 examples/s]\u001b[A\n",
            "Map:  13% 660/5000 [00:01<00:09, 438.12 examples/s]\u001b[A\n",
            "Map:  15% 728/5000 [00:01<00:09, 438.87 examples/s]\u001b[A\n",
            "Map:  16% 776/5000 [00:01<00:10, 402.90 examples/s]\u001b[A\n",
            "Map:  16% 819/5000 [00:01<00:10, 407.66 examples/s]\u001b[A\n",
            "model.safetensors:   5% 26.0M/496M [00:02<00:42, 11.0MB/s]\n",
            "Map:  18% 906/5000 [00:01<00:10, 375.84 examples/s]\u001b[A\n",
            "model.safetensors:  19% 93.1M/496M [00:02<00:09, 43.7MB/s]\n",
            "model.safetensors:  73% 361M/496M [00:03<00:00, 182MB/s]\n",
            "Map:  21% 1052/5000 [00:03<00:31, 125.96 examples/s]\u001b[A\n",
            "Map:  23% 1127/5000 [00:03<00:20, 185.36 examples/s]\u001b[A\n",
            "Map:  24% 1199/5000 [00:03<00:15, 249.17 examples/s]\u001b[A\n",
            "Map:  25% 1271/5000 [00:03<00:11, 317.51 examples/s]\u001b[A\n",
            "Map:  27% 1345/5000 [00:03<00:09, 389.10 examples/s]\u001b[A\n",
            "Map:  28% 1421/5000 [00:03<00:07, 461.41 examples/s]\u001b[A\n",
            "Map:  30% 1490/5000 [00:03<00:06, 510.62 examples/s]\u001b[A\n",
            "Map:  31% 1563/5000 [00:03<00:06, 561.68 examples/s]\u001b[A\n",
            "Map:  33% 1634/5000 [00:04<00:05, 598.05 examples/s]\u001b[A\n",
            "Map:  35% 1737/5000 [00:04<00:05, 625.20 examples/s]\u001b[A\n",
            "Map:  36% 1810/5000 [00:04<00:04, 648.16 examples/s]\u001b[A\n",
            "Map:  38% 1880/5000 [00:04<00:04, 659.15 examples/s]\u001b[A\n",
            "Map:  40% 1980/5000 [00:04<00:04, 657.51 examples/s]\u001b[A\n",
            "Map:  41% 2067/5000 [00:04<00:06, 472.83 examples/s]\u001b[A\n",
            "Map:  43% 2139/5000 [00:04<00:05, 518.64 examples/s]\u001b[A\n",
            "Map:  44% 2208/5000 [00:05<00:05, 553.74 examples/s]\u001b[A\n",
            "Map:  45% 2271/5000 [00:05<00:04, 569.83 examples/s]\u001b[A\n",
            "Map:  47% 2344/5000 [00:05<00:04, 607.33 examples/s]\u001b[A\n",
            "Map:  48% 2416/5000 [00:05<00:04, 635.42 examples/s]\u001b[A\n",
            "Map:  50% 2517/5000 [00:05<00:03, 644.24 examples/s]\u001b[A\n",
            "Map:  52% 2585/5000 [00:05<00:03, 651.02 examples/s]\u001b[A\n",
            "Map:  53% 2657/5000 [00:05<00:03, 666.39 examples/s]\u001b[A\n",
            "Map:  55% 2736/5000 [00:05<00:03, 613.52 examples/s]\u001b[A\n",
            "Map:  56% 2802/5000 [00:05<00:03, 623.10 examples/s]\u001b[A\n",
            "Map:  57% 2870/5000 [00:06<00:03, 635.27 examples/s]\u001b[A\n",
            "Map:  59% 2964/5000 [00:06<00:03, 628.23 examples/s]\u001b[A\n",
            "Map:  61% 3028/5000 [00:06<00:04, 425.94 examples/s]\u001b[A\n",
            "Map:  62% 3094/5000 [00:06<00:04, 470.78 examples/s]\u001b[A\n",
            "Map:  63% 3168/5000 [00:06<00:03, 528.74 examples/s]\u001b[A\n",
            "Map:  65% 3237/5000 [00:06<00:03, 565.10 examples/s]\u001b[A\n",
            "Map:  66% 3310/5000 [00:06<00:02, 604.02 examples/s]\u001b[A\n",
            "Map:  68% 3377/5000 [00:07<00:02, 617.82 examples/s]\u001b[A\n",
            "Map:  69% 3450/5000 [00:07<00:02, 646.05 examples/s]\u001b[A\n",
            "model.safetensors: 100% 496M/496M [00:07<00:00, 62.6MB/s]\n",
            "\n",
            "Map:  72% 3617/5000 [00:07<00:02, 524.32 examples/s]\u001b[A\n",
            "Map:  74% 3685/5000 [00:07<00:02, 557.37 examples/s]\u001b[A\n",
            "Map:  75% 3750/5000 [00:07<00:02, 576.28 examples/s]\u001b[A\n",
            "Map:  76% 3821/5000 [00:07<00:01, 608.37 examples/s]\u001b[A\n",
            "Map:  78% 3892/5000 [00:07<00:01, 631.81 examples/s]\u001b[A\n",
            "Map:  79% 3969/5000 [00:07<00:01, 665.88 examples/s]\u001b[A\n",
            "Map:  81% 4060/5000 [00:08<00:02, 461.84 examples/s]\u001b[A\n",
            "Map:  83% 4133/5000 [00:08<00:01, 514.64 examples/s]\u001b[A\n",
            "Map:  84% 4203/5000 [00:08<00:01, 554.83 examples/s]\u001b[A\n",
            "Map:  86% 4277/5000 [00:08<00:01, 596.58 examples/s]\u001b[A\n",
            "Map:  87% 4352/5000 [00:08<00:01, 632.06 examples/s]\u001b[A\n",
            "Map:  89% 4427/5000 [00:08<00:00, 660.38 examples/s]\u001b[A\n",
            "Map:  91% 4531/5000 [00:08<00:00, 668.30 examples/s]\u001b[A\n",
            "Map:  93% 4637/5000 [00:09<00:00, 678.94 examples/s]\u001b[A\n",
            "Map:  94% 4714/5000 [00:09<00:00, 698.15 examples/s]\u001b[A\n",
            "Map:  96% 4811/5000 [00:09<00:00, 677.56 examples/s]\u001b[A\n",
            "Map:  98% 4891/5000 [00:09<00:00, 627.08 examples/s]\u001b[A\n",
            "Map: 100% 5000/5000 [00:10<00:00, 499.04 examples/s]\n",
            "{'loss': '1.843', 'grad_norm': '3.383', 'learning_rate': '4.604e-05', 'epoch': '0.08'}\n",
            "{'loss': '0.5143', 'grad_norm': '3.02', 'learning_rate': '4.204e-05', 'epoch': '0.16'}\n",
            "{'loss': '0.5055', 'grad_norm': '2.85', 'learning_rate': '3.804e-05', 'epoch': '0.24'}\n",
            "{'loss': '0.5042', 'grad_norm': '2.509', 'learning_rate': '3.404e-05', 'epoch': '0.32'}\n",
            "{'loss': '0.4913', 'grad_norm': '2.299', 'learning_rate': '3.004e-05', 'epoch': '0.4'}\n",
            "{'loss': '0.485', 'grad_norm': '3.102', 'learning_rate': '2.604e-05', 'epoch': '0.48'}\n",
            "{'loss': '0.4857', 'grad_norm': '1.892', 'learning_rate': '2.204e-05', 'epoch': '0.56'}\n",
            "{'loss': '0.4967', 'grad_norm': '2.375', 'learning_rate': '1.804e-05', 'epoch': '0.64'}\n",
            "{'loss': '0.4709', 'grad_norm': '1.766', 'learning_rate': '1.404e-05', 'epoch': '0.72'}\n",
            "{'loss': '0.4681', 'grad_norm': '2.346', 'learning_rate': '1.004e-05', 'epoch': '0.8'}\n",
            " 80% 1000/1250 [02:38<00:35,  6.97it/s]\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:13<00:00, 13.02s/it]\n",
            "{'loss': '0.4908', 'grad_norm': '2.646', 'learning_rate': '6.04e-06', 'epoch': '0.88'}\n",
            "{'loss': '0.4724', 'grad_norm': '1.899', 'learning_rate': '2.04e-06', 'epoch': '0.96'}\n",
            "100% 1250/1250 [04:21<00:00,  6.63it/s]\n",
            "Writing model shards:   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Writing model shards: 100% 1/1 [00:15<00:00, 15.24s/it]\n",
            "{'train_runtime': '337.9', 'train_samples_per_second': '14.8', 'train_steps_per_second': '3.7', 'train_loss': '0.5972', 'epoch': '1'}\n",
            "100% 1250/1250 [05:37<00:00,  3.70it/s]\n",
            "Writing model shards: 100% 1/1 [00:14<00:00, 14.91s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **학습 완료 모델 로드 및 요약 테스트 (추론)**\n",
        "\n",
        "**1. 파인튜닝된 모델 불러오기**\n",
        "* 학습 후 `./final_model` 폴더에 저장해둔 **최종 모델과 토크나이저를 로드**\n",
        "\n",
        "**2. 검증 데이터(Validation) 준비**\n",
        "* 테스트를 위해 `valid_processed.json` 파일을 로드\n",
        "* 데이터 중 **첫 번째 문서의 원문**(`input_text`)을 가져와 테스트 문장으로 사용\n",
        "\n",
        "**3. 텍스트 추론 및 요약 생성**\n",
        "* 원문을 모델이 이해할 수 있도록 토크나이징 (최대 512 토큰 제한)\n",
        "* **`model.generate()`** 함수로 요약 텍스트를 생성 (`num_beams=4`를 적용해 더 자연스럽고 퀄리티 높은 문장을 탐색, 최대 128 토큰)\n",
        "* 생성된 결과값(ID)을 사람이 읽을 수 있는 일반 텍스트로 디코딩 (특수 토큰 제거)\n",
        "\n",
        "**4. 결과 확인**\n",
        "* 입력한 원문의 앞부분과 모델이 **직접 생성한 요약문을 화면에 출력하여 성능을 확인**"
      ],
      "metadata": {
        "id": "1g1Q1Ci-lq2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# 학습 끝난 모델 불러오기\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./final_model\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"./final_model\")\n",
        "\n",
        "\n",
        "# 예시: validation 데이터에서 첫 문서 가져오기\n",
        "import json\n",
        "\n",
        "with open(\"valid_processed.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    valid_data = json.load(f)\n",
        "\n",
        "text = valid_data[0][\"input_text\"]  # 첫 번째 문서 원문\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "summary_ids = model.generate(**inputs, max_length=128, num_beams=4)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"원문:\", text[:200], \"...\")  # 길면 앞부분만\n",
        "print(\"생성 요약:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "3e2b14151a744f1f87d6500efdb9f8ee",
            "1d161b1e46f74e79a20c7caa18d34130",
            "f8ff2b0192374e309a6e1e98d5324fe8",
            "765fae00238a4530b5efa788caea2913",
            "3430f65319184018828bc358dd1082c1",
            "ae2d0b18f37149ab87ba735a14e5a570",
            "ee36788fbcb84760bbe5ab00baee6560",
            "7aef4eb6e347427ba5bd1506d7eb79ab",
            "0c0f10555c3f461aa519cc6330171a75",
            "74ad81dc9ad54d82aa18281dad4fe4b8",
            "04db678ec92a41f3a01e188ac11f3610"
          ]
        },
        "id": "9a6t6H2Klu4-",
        "outputId": "66823ad2-6381-4d7b-dc3f-ca60068a5e99"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/262 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e2b14151a744f1f87d6500efdb9f8ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원문: [ 박재원 기자 ] '대한민국 5G 홍보대사'를 자처한 문재인 대통령은 \"넓고, 체증 없는 '통신 고속도로'가 5G\"라며 \"대한민국의 대전환이 이제 막 시작됐다\"고 기대감을 높였다. 문 대통령은 8일 서울 올림픽공원에서 열린 5G플러스 전략발표에 참석해 \"5G 시대는 우리가 생각하고, 만들면 그것이 세계 표준이 되는 시대\"라며 \"5G는 대한민국 혁신성장의  ...\n",
            "생성 요약: '대한민국 5G 홍보대사'를 자처한 문재인 대통령은 5G가 4차 산업혁명 시대의 고속도로가 돼 새로운 기회를 열어 줄 것이라고 강조했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **검증 데이터 전체 요약 및 ROUGE 평가 (CSV 저장)**\n",
        "\n",
        "**1. 모델 로드 및 평가 환경 세팅**\n",
        "* 학습이 끝난 최종 모델(`./final_model`)을 불러와 **GPU(`cuda`)에 할당**\n",
        "* 모델을 평가 모드(`model.eval()`)로 전환하고 검증용 데이터(`valid_processed.json`)를 로드\n",
        "\n",
        "**2. 배치 단위 요약 생성 (속도 최적화)**\n",
        "* 추론 속도를 높이기 위해 **배치 사이즈 16, 빔 서치(`num_beams`) 1, 최대 입력 길이 384**로 세팅\n",
        "* `tqdm`으로 진행률을 확인하며, 메모리 절약을 위해 **`torch.no_grad()`** 상태에서 배치 단위로 요약문을 생성\n",
        "\n",
        "**3. 모델 성능 평가 (ROUGE Score)**\n",
        "* Hugging Face의 **`evaluate` 라이브러리를 사용해 ROUGE 점수를 계산**\n",
        "* 실제 정답 요약문(Reference)과 모델이 생성한 요약문(Prediction)을 비교해 객관적인 요약 성능을 수치화\n",
        "\n",
        "**4. 평가 결과 CSV 파일 저장**\n",
        "* 원문, 정답 요약문, 생성 요약문을 묶어 **Pandas DataFrame**으로 변환\n",
        "* 엑셀에서 한글 깨짐을 방지하기 위해 `encoding=\"utf-8-sig\"`를 적용하여, **`validation_summaries.csv` 파일로 저장**"
      ],
      "metadata": {
        "id": "bGWHswwYmiCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 validation 요약 + ROUGE 계산 + CSV 저장\n",
        "\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "\n",
        "# 1. 모델 로드\n",
        "model_path = \"./final_model\"  # 학습한 모델 경로\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2. validation 데이터 로드\n",
        "with open(\"valid_processed.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    valid_data = json.load(f)\n",
        "\n",
        "# 3. 배치 요약 생성 (속도 최적화)\n",
        "batch_size = 16       # GPU VRAM 여유 있으면 32 가능\n",
        "num_beams = 1         # 속도 위해 빔서치 1\n",
        "max_input_len = 384   # 입력 토큰 길이 줄임\n",
        "max_output_len = 128\n",
        "\n",
        "generated_summaries = []\n",
        "\n",
        "for i in tqdm(range(0, len(valid_data), batch_size), desc=\"Generating summaries\"):\n",
        "    batch = valid_data[i:i+batch_size]\n",
        "    texts = [doc[\"input_text\"] for doc in batch]\n",
        "\n",
        "    # 배치 토크나이징\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=max_input_len,\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_output_len,\n",
        "            num_beams=num_beams,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    batch_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
        "    generated_summaries.extend(batch_summaries)\n",
        "\n",
        "print(\"배치 요약 완료, 총 생성 개수:\", len(generated_summaries))\n",
        "\n",
        "# 4. ROUGE 계산\n",
        "rouge = evaluate.load(\"rouge\")  # ← datasets 대신 evaluate 사용\n",
        "\n",
        "references = [doc[\"target_text\"] for doc in valid_data]\n",
        "results = rouge.compute(predictions=generated_summaries, references=references)\n",
        "print(\"🔹 ROUGE 결과:\", results)\n",
        "\n",
        "# 5. 결과 CSV 저장\n",
        "df = pd.DataFrame({\n",
        "    \"input_text\": [doc[\"input_text\"] for doc in valid_data],\n",
        "    \"target_text\": references,\n",
        "    \"generated_summary\": generated_summaries\n",
        "})\n",
        "\n",
        "df.to_csv(\"validation_summaries.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "print(\"결과 CSV 저장 완료: validation_summaries.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c04ba37ccfca48809be0dd0443c97bd3",
            "3eea3c4772334db6a05002fc3f96c591",
            "397dfd3773924b38afbb4dfaba9fd96e",
            "4774b650bcdf403e890cc753000ad55b",
            "6c947b7fa2ff46a1a43ee706ffa44761",
            "4d01386d12a24b19853d483a24ade373",
            "a6f5cec0b0414564bd30bd98671c698a",
            "1986212963e94b54b7d74683554a5455",
            "b8d9756d78ca49e98c43b84dd35764a3",
            "c67f8d94ceca4d35b872527761782c15",
            "f6326355d27840378320f5e474055539",
            "aad683e0564e41f3a5f6af1dfcc549a1",
            "a492fd0b1972498cbb123d839e6b1916",
            "265c3de849244e438f93b09e0d918c25",
            "9f2795732bde46f2b73f10e0c8cb07f7",
            "a32084251b5b40e4aba109d00cce21f0",
            "9832af1cf1d7463db4d90c4d3d6781f7",
            "3bb5f030b4554520becf7b24974d1479",
            "c92a1cee4cc84c36bd963be46c77d97c",
            "f7a48290d9c549fe84665938493c5959",
            "9169401f166248fb8c9f22eabdbc742e",
            "a663ddf5867b44e7b313d2a6a6a90a73"
          ]
        },
        "id": "H6A5n08XvL5h",
        "outputId": "08741382-451c-4849-d89b-b946e0ab006e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.24.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.7.0->evaluate) (0.24.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->huggingface-hub>=0.7.0->evaluate) (0.1.2)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m721.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.3)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7ea97f271bf10e20cd1421041b98925ec1774c01d4b99e230f28fb8632437256\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/262 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c04ba37ccfca48809be0dd0443c97bd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "Generating summaries:   0%|          | 0/1883 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating summaries: 100%|██████████| 1883/1883 [45:46<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 배치 요약 완료! 총 생성 개수: 30122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aad683e0564e41f3a5f6af1dfcc549a1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔹 ROUGE 결과: {'rouge1': np.float64(0.43219674597645785), 'rouge2': np.float64(0.17804824548746), 'rougeL': np.float64(0.41315382344596097), 'rougeLsum': np.float64(0.4133655222460041)}\n",
            "✅ 결과 CSV 저장 완료: validation_summaries.csv\n"
          ]
        }
      ]
    }
  ]
}